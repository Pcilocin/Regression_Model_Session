# ==============================================================================
# CLASSIFICATION_SHORT.py
# ------------------------------------------------------------------------------
# –ó–ê–î–ê–ß–ê: –û–±—É—á–∏—Ç—å, –æ—Ü–µ–Ω–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –†–ï–ì–†–ï–°–°–ò–û–ù–ù–£–Æ –º–æ–¥–µ–ª—å-"–†–∞–∑–≤–µ–¥—á–∏–∫–∞",
# –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –†–ê–ó–ú–ï–† –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–µ—Ç–∞–ø–∞ "London sweeps Asia".
# ==============================================================================

import pandas as pd
import numpy as np
import optuna
import os
import random
import warnings
from dotenv import load_dotenv
import plotly.graph_objects as go
import json

from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ù–û–í–´–ô –∫–ª–∞—Å—Å –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
from trading_tools import (
    LiquidityMLModel,
    FeatureEngineSMC,
    prepare_master_dataframe,
    START_DATE,
    TICKER,
    safe_ticker,
    DOWNLOAD_DATA
)

warnings.filterwarnings('ignore')

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò –ò –ö–û–ù–°–¢–ê–ù–¢–´ ---
MODEL_TYPE = 'SHORT'
N_TRAILS_MAE = 200  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–µ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏


def set_seed(seed=33):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    print(f"‚úÖ –°–ª—É—á–∞–π–Ω–æ—Å—Ç—å –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞ —Å seed = {seed}")


# --- 2. –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò (MAE) ---
def objective_mae_cv(trial, X_train_full, y_train_full):
    """
    –ò—â–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –†–ï–ì–†–ï–°–°–ò–û–ù–ù–û–ô –ú–û–î–ï–õ–ò –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É MAE.
    """
    model_params = {
        'objective': 'regression_l1',  # L1 loss = MAE
        'metric': 'mae',
        'random_state': 33,
        'verbosity': -1,
        'n_jobs': -1,
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'num_leaves': trial.suggest_int('num_leaves', 20, 60),
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
    }

    tscv = TimeSeriesSplit(n_splits=5)
    mae_scores = []

    for train_index, val_index in tscv.split(X_train_full):
        X_train_fold, X_val_fold = X_train_full.iloc[train_index], X_train_full.iloc[val_index]
        y_train_fold, y_val_fold = y_train_full.iloc[train_index], y_train_full.iloc[val_index]

        model = LiquidityMLModel(params=model_params)
        model.train(X_train_fold, y_train_fold)

        predictions = model.predict(X_val_fold)
        score = mean_absolute_error(y_val_fold, predictions)
        mae_scores.append(score)

    return np.mean(mae_scores)


# --- 3. –û–°–ù–û–í–ù–û–ô –ë–õ–û–ö –í–´–ü–û–õ–ù–ï–ù–ò–Ø ---
if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    set_seed(33)
    load_dotenv()

    # --- –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
    print("\n--- –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---")
    final_df, df_30m, df_15m, df_5m, df_1m = prepare_master_dataframe(START_DATE, TICKER, DOWNLOAD_DATA)

    # --- –≠–¢–ê–ü 2: –ò–ù–ñ–ò–ù–ò–†–ò–ù–ì –ü–†–ò–ó–ù–ê–ö–û–í –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ---
    print("\n--- –≠–¢–ê–ü 2: –ò–ù–ñ–ò–ù–ò–†–ò–ù–ì –ü–†–ò–ó–ù–ê–ö–û–í –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –î–ê–ù–ù–´–• ---")

    # 2.1. –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏. –í 'y' –±—É–¥–µ—Ç –º–Ω–æ–≥–æ NaN, —Ç–∞–∫ –∫–∞–∫ –º—ã —Ä–∞–∑–º–µ—á–∞–ª–∏ —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–µ—Ç–∞–ø—ã.
    feature_engine = FeatureEngineSMC(final_df, ltf_df_30m=df_30m, ltf_df_5m=df_5m, ltf_df_1m=df_1m)
    X, y, df_with_features = feature_engine.run(model_type=MODEL_TYPE, create_target=True)

    # 2.2. –§–∏–ª—å—Ç—Ä—É–µ–º "–º—É—Å–æ—Ä": –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏.
    print(f"\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(X)} —Å–≤–µ—á–µ–π.")

    full_dataset = X.copy()
    full_dataset['target'] = y
    full_dataset.dropna(subset=['target'], inplace=True)  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–µ—Ç–∞–ø–∞

    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(full_dataset)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Å—Ç–∞–ª–æ—Å—å –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–±–æ—Ç—ã
    if len(full_dataset) < 50:
        print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
        print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ 'min_target_r' –≤ —Ñ–∞–π–ª–µ trading_tools.py.")
        exit()

    # –†–∞–∑–¥–µ–ª—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ (X) –∏ —Ü–µ–ª—å (y)
    X_filtered = full_dataset.drop(columns=['target'])
    y_filtered = full_dataset['target']

    # 2.3. –î–µ–ª–∏–º –û–ß–ò–©–ï–ù–ù–´–ï –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏.
    # –≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞–º –Ω—É–∂–Ω–æ.
    X_train, X_test, y_train, y_test = train_test_split(
        X_filtered, y_filtered, test_size=0.2, shuffle=False, random_state=33
    )
    print(f"–î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã: {len(X_train)} –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, {len(X_test)} –¥–ª—è —Ç–µ—Å—Ç–∞.")

    # --- –≠–¢–ê–ü 3: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í (OPTUNA) ---
    print(f"\n--- –≠–¢–ê–ü 3: –ü–û–ò–°–ö –õ–£–ß–®–ï–ô –†–ï–ì–†–ï–°–°–ò–û–ù–ù–û–ô –ú–û–î–ï–õ–ò ({MODEL_TYPE}) ---")

    study = optuna.create_study(direction='minimize')  # –ú—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É (MAE)
    # –ü–µ—Ä–µ–¥–∞–µ–º –≤ Optuna –¢–û–õ–¨–ö–û –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É (X_train, y_train) –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    study.optimize(lambda trial: objective_mae_cv(trial, X_train, y_train), n_trials=N_TRAILS_MAE)

    best_model_hyperparams = study.best_params
    print("\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("üî• –õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –†–ï–ì–†–ï–°–°–û–†–ê –Ω–∞–π–¥–µ–Ω—ã:", best_model_hyperparams)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ñ–∞–π–ª
    params_filename = f"regression_params_{MODEL_TYPE}_{safe_ticker}.json"
    with open(params_filename, 'w') as f:
        json.dump(best_model_hyperparams, f, indent=4)
    print(f"üíæ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {params_filename}")

    # --- –≠–¢–ê–ü 4: –§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ò –û–¶–ï–ù–ö–ê ---
    print("\n--- –≠–¢–ê–ü 4: –§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ò –û–¶–ï–ù–ö–ê –†–ï–ì–†–ï–°–°–û–†–ê ---")

    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    final_model = LiquidityMLModel(params=best_model_hyperparams)

    print("–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –∏ Optuna (X_train, y_train)
    final_model.train(X_train, y_train)

    model_filename = f"regressor_model_{MODEL_TYPE}_{safe_ticker}.pkl"
    final_model.save_model(model_filename)

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ù–ï–í–ò–î–ò–ú–´–• –¥–∞–Ω–Ω—ã—Ö (X_test, y_test), –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∞
    print("\n–û—Ü–µ–Ω–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö (–Ω–µ–≤–∏–¥–∏–º—ã—Ö) –¥–∞–Ω–Ω—ã—Ö:")
    results_df = final_model.evaluate(X_test, y_test)

    # --- –≠–¢–ê–ü 5: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
    print("\n--- –≠–¢–ê–ü 5: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –¢–û–ß–ù–û–°–¢–ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ---")
    fig = go.Figure()

    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å—Å–µ—è–Ω–∏—è: –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    fig.add_trace(go.Scatter(
        x=results_df['y_true'],
        y=results_df['y_pred'],
        mode='markers',
        name='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è',
        marker=dict(color='rgba(100, 181, 246, 0.7)', line=dict(width=1, color='DarkSlateGrey'))
    ))

    # –õ–∏–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –∏–¥–µ–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–≥–¥–µ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å = –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
    fig.add_trace(go.Scatter(
        x=[results_df['y_true'].min(), results_df['y_true'].max()],
        y=[results_df['y_true'].min(), results_df['y_true'].max()],
        mode='lines',
        name='–ò–¥–µ–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ',
        line=dict(color='crimson', width=2, dash='dash')
    ))

    fig.update_layout(
        title=f'–¢–æ—á–Ω–æ—Å—Ç—å –†–µ–≥—Ä–µ—Å—Å–æ—Ä–∞: –†–µ–∞–ª—å–Ω—ã–π vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π R ({TICKER})',
        xaxis_title='–†–µ–∞–ª—å–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ (–≤ R)',
        yaxis_title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ (–≤ R)',
        template='plotly_dark'
    )

    plot_filename = f'regression_accuracy_plot_{MODEL_TYPE}_{safe_ticker}.html'
    fig.write_html(plot_filename)
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {plot_filename}")

