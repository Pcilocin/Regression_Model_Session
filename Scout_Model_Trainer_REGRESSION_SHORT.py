# ==============================================================================
# Scout_Model_Trainer_REGRESSION_SHORT.py
# ------------------------------------------------------------------------------
# –ó–ê–î–ê–ß–ê: –û–±—É—á–∏—Ç—å, –æ—Ü–µ–Ω–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –†–ï–ì–†–ï–°–°–ò–û–ù–ù–£–Æ –º–æ–¥–µ–ª—å-"–†–∞–∑–≤–µ–¥—á–∏–∫–∞",
# –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –†–ê–ó–ú–ï–† –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–µ—Ç–∞–ø–∞ "London sweeps Asia".
# ==============================================================================

import pandas as pd
import numpy as np
import optuna
import os
import random
import warnings
from dotenv import load_dotenv
import plotly.graph_objects as go
import json

from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ù–û–í–´–ô –∫–ª–∞—Å—Å –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
from trading_tools import (
    LiquidityMLRegressor,
    FeatureEngineSMC,
    prepare_master_dataframe,
    START_DATE,
    TICKER,
    safe_ticker,
    DOWNLOAD_DATA
)

warnings.filterwarnings('ignore')

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò –ò –ö–û–ù–°–¢–ê–ù–¢–´ ---
MODEL_TYPE = 'SHORT'
N_TRAILS_MAE = 75  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–µ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏


def set_seed(seed=33):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    print(f"‚úÖ –°–ª—É—á–∞–π–Ω–æ—Å—Ç—å –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞ —Å seed = {seed}")


# --- 2. –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò (MAE) ---
def objective_mae_cv(trial, X_train_full, y_train_full):
    """
    –ò—â–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –†–ï–ì–†–ï–°–°–ò–û–ù–ù–û–ô –ú–û–î–ï–õ–ò –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É MAE.
    """
    model_params = {
        'objective': 'regression_l1',  # L1 loss = MAE
        'metric': 'mae',
        'random_state': 33,
        'verbosity': -1,
        'n_jobs': -1,
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'num_leaves': trial.suggest_int('num_leaves', 20, 60),
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
    }

    tscv = TimeSeriesSplit(n_splits=5)
    mae_scores = []

    for train_index, val_index in tscv.split(X_train_full):
        X_train_fold, X_val_fold = X_train_full.iloc[train_index], X_train_full.iloc[val_index]
        y_train_fold, y_val_fold = y_train_full.iloc[train_index], y_train_full.iloc[val_index]

        model = LiquidityMLRegressor(params=model_params)
        model.train(X_train_fold, y_train_fold)

        predictions = model.predict(X_val_fold)
        score = mean_absolute_error(y_val_fold, predictions)
        mae_scores.append(score)

    return np.mean(mae_scores)


# --- 3. –û–°–ù–û–í–ù–û–ô –ë–õ–û–ö –í–´–ü–û–õ–ù–ï–ù–ò–Ø ---
if __name__ == "__main__":
    set_seed(33)
    load_dotenv()

    # --- –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ---
    final_df, df_30m, df_15m, df_5m, df_1m = prepare_master_dataframe(START_DATE, TICKER, DOWNLOAD_DATA)

    # --- –≠–¢–ê–ü 2: –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ---
    print("\n--- –≠–¢–ê–ü 2: –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –î–ê–ù–ù–´–• ---")
    feature_engine = FeatureEngineSMC(final_df, df_30m, df_5m, df_1m)
    X, y, _ = feature_engine.run(model_type=MODEL_TYPE, create_target=True)

    # –ö–õ–Æ–ß–ï–í–û–ô –®–ê–ì: –ú—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –¢–û–õ–¨–ö–û –Ω–∞ —Ç–µ—Ö –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ –±—ã–ª —Å–µ—Ç–∞–ø
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ y = NaN
    valid_indices = y.dropna().index
    X_filtered = X.loc[valid_indices]
    y_filtered = y.loc[valid_indices]

    print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(X_filtered)} —Å–æ–±—ã—Ç–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.")

    if len(X_filtered) < 50:
        print("‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        exit()

    X_train_full, X_test, y_train_full, y_test = train_test_split(
        X_filtered, y_filtered, test_size=0.3, shuffle=False
    )

    # --- –≠–¢–ê–ü 3: –ü–û–ò–°–ö –õ–£–ß–®–ï–ô –†–ï–ì–†–ï–°–°–ò–û–ù–ù–û–ô –ú–û–î–ï–õ–ò ---
    print(f"\n--- –≠–¢–ê–ü 3: –ü–û–ò–°–ö –õ–£–ß–®–ï–ô –†–ï–ì–†–ï–°–°–ò–û–ù–ù–û–ô –ú–û–î–ï–õ–ò ({MODEL_TYPE}) ---")
    study = optuna.create_study(direction='minimize')  # –ú–ò–ù–ò–ú–ò–ó–ò–†–£–ï–ú –æ—à–∏–±–∫—É
    study.optimize(lambda trial: objective_mae_cv(trial, X_train_full, y_train_full), n_trials=N_TRAILS_MAE)

    best_model_hyperparams = study.best_params
    print("\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("üî• –õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –†–ï–ì–†–ï–°–°–û–†–ê –Ω–∞–π–¥–µ–Ω—ã:", best_model_hyperparams)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ JSON
    params_filename = f"regression_params_{MODEL_TYPE}_{safe_ticker}.json"
    with open(params_filename, 'w') as f:
        json.dump(best_model_hyperparams, f, indent=4)
    print(f"üíæ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {params_filename}")

    # --- –≠–¢–ê–ü 4: –§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ò –û–¶–ï–ù–ö–ê ---
    print("\n--- –≠–¢–ê–ü 4: –§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ò –û–¶–ï–ù–ö–ê –†–ï–ì–†–ï–°–°–û–†–ê ---")
    final_model = LiquidityMLRegressor(params=best_model_hyperparams)
    print("–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    final_model.train(X_train_full, y_train_full)

    model_filename = f"regressor_model_{MODEL_TYPE}_{safe_ticker}.pkl"
    final_model.save_model(model_filename)

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –Ω–µ–≤–∏–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö (X_test)
    results_df = final_model.evaluate(X_test, y_test)

    # --- –≠–¢–ê–ü 5: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
    print("\n--- –≠–¢–ê–ü 5: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –¢–û–ß–ù–û–°–¢–ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ---")
    fig = go.Figure()

    # Scatter plot: –†–µ–∞–ª—å–Ω–æ—Å—Ç—å vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    fig.add_trace(go.Scatter(
        x=results_df['y_true'],
        y=results_df['y_pred'],
        mode='markers',
        name='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è',
        marker=dict(color='rgba(100, 181, 246, 0.7)', line=dict(width=1, color='DarkSlateGrey'))
    ))

    # –õ–∏–Ω–∏—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (y=x)
    fig.add_trace(go.Scatter(
        x=[results_df['y_true'].min(), results_df['y_true'].max()],
        y=[results_df['y_true'].min(), results_df['y_true'].max()],
        mode='lines',
        name='–ò–¥–µ–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ',
        line=dict(color='crimson', width=2, dash='dash')
    ))

    fig.update_layout(
        title=f'–¢–æ—á–Ω–æ—Å—Ç—å –†–µ–≥—Ä–µ—Å—Å–æ—Ä–∞: –†–µ–∞–ª—å–Ω—ã–π vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π R ({TICKER})',
        xaxis_title='–†–µ–∞–ª—å–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ (–≤ R)',
        yaxis_title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ (–≤ R)',
        template='plotly_dark'
    )

    plot_filename = f'regression_accuracy_plot_{MODEL_TYPE}.html'
    fig.write_html(plot_filename)
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {plot_filename}")
